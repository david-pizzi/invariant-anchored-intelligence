# IAI Bandit Pilot PoC

A minimal, runnable proof of concept for **Pilot 0**: Invariant-Anchored Recursive Optimisation in a Multi-Armed Bandit environment.

## What this includes
- Bernoulli K-armed bandit with optional **distribution drift**
- Baselines: Îµ-greedy, UCB1, Thompson Sampling
- IAI meta-policy: **policy selector** that recursively improves by selecting among base policies using an external evaluator's metrics (reward/regret)
- External evaluator computes metrics (cumulative reward, regret, stability, drift recovery) and writes CSV logs

## Quick start

```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux: source .venv/bin/activate

pip install -r requirements.txt

python run_experiment.py --k 10 --steps 50000 --seeds 10 --drift_step 20000 --outdir runs/demo
```

This will create:
- `runs/demo/summary.csv` (one row per run/system)
- `runs/demo/trajectories/*.csv` (step-level logs)
- `runs/demo/plots/*.png` (regret and reward curves)

## Notes on "invariants"
- The reward is generated by the environment (immutable to the agent).
- The evaluator computes regret from true means (not visible to the agent).
- The IAI selector may adapt only its **selection weights** across a fixed menu of strategies.
